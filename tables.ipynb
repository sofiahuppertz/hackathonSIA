{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVdtcEDEW9hU"
   },
   "source": [
    "# Notebook Application for Weekly Research on Daily Portfolio Companies using Tavily and LangGraph Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langgraph langchain_openai langchain_core tavily-python langchain-community fpdf\n",
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the lastest version of Tavily for new extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated, Literal, Dict, Union, Optional \n",
    "from datetime import datetime\n",
    "\n",
    "from tavily import AsyncTavilyClient, TavilyClient\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import AnyMessage, AIMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from urllib.parse import urlparse\n",
    "from langgraph.graph import StateGraph, START, END, add_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set API KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration initiale de l'environnement\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "        \n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"report-mAIstro\"\n",
    "_set_env(\"TAVILY_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(model=\"meta/llama-3.3-70b-instruct\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Generating PDF Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fpdf import FPDF\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font(\"Arial\", \"B\", 12)\n",
    "        self.cell(0, 10, \"\", 0, 1, \"C\")\n",
    "\n",
    "    def footer(self):\n",
    "        self.set_y(-15)\n",
    "        self.set_font(\"Arial\", \"I\", 8)\n",
    "        self.cell(0, 10, f\"Page {self.page_no()}\", 0, 0, \"C\")\n",
    "\n",
    "def sanitize_content(content):\n",
    "    try:\n",
    "        # Use 'utf-8' encoding to handle Unicode characters\n",
    "        encoded_content = content.encode('utf-8', 'ignore').decode('utf-8')\n",
    "        return encoded_content\n",
    "    except UnicodeEncodeError as e:\n",
    "        print(f\"Encoding error: {e}\")\n",
    "\n",
    "        # Remove problematic characters using 'ascii' encoding\n",
    "        sanitized_content = content.encode('ascii', 'ignore').decode('ascii')\n",
    "        return sanitized_content\n",
    "\n",
    "def replace_problematic_characters(content):\n",
    "    # Replace or remove problematic characters\n",
    "    replacements = {\n",
    "        '\\u2013': '-',  # en dash to hyphen\n",
    "        '\\u2014': '--',  # en dash to double hyphen\n",
    "        '\\u2018': \"'\",  # left single quotation mark to apostrophe\n",
    "        '\\u2019': \"'\",  # right single quotation mark to apostrophe\n",
    "        '\\u201c': '\"',  # left double quotation mark to double quote\n",
    "        '\\u201d': '\"',  # right double quotation mark to double quote\n",
    "        '\\u2026': '...',  # horizontal ellipsis\n",
    "        '\\u2010': '-',   # dash\n",
    "        '\\u2022': '*',   # bullet\n",
    "        '\\u2122': 'TM',  # TradeMark Symbol\n",
    "        '\\u20ac': '€'  # Euro Symbol\n",
    "    }\n",
    "\n",
    "    for char, replacement in replacements.items():\n",
    "        content = content.replace(char, replacement)\n",
    "\n",
    "    return content\n",
    "\n",
    "def generate_pdf_from_md(content, filename='output.pdf'):\n",
    "    try:\n",
    "        pdf = PDF()\n",
    "        pdf.add_page()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "        pdf.set_font('Arial', '', 12)\n",
    "\n",
    "        sanitized_content = sanitize_content(content)\n",
    "        sanitized_content = replace_problematic_characters(sanitized_content)\n",
    "\n",
    "        lines = sanitized_content.split('\\n')\n",
    "\n",
    "        for line in lines:\n",
    "            if line.startswith('#'):\n",
    "                header_level = min(line.count('#'), 4)\n",
    "                header_text = re.sub(r'\\*{2,}', '', line.strip('# ').strip())\n",
    "                pdf.set_font('Arial', 'B', 12 + (4 - header_level) * 2)\n",
    "                pdf.multi_cell(0, 10, header_text)\n",
    "                pdf.set_font('Arial', '', 12)\n",
    "            else:\n",
    "                parts = re.split(r'(\\*\\*\\*.*?\\*\\*\\*|\\*\\*.*?\\*\\*|\\*.*?\\*|\\[.*?\\]\\(.*?\\)|\\([^ ]+?\\))', line)\n",
    "                for part in parts:\n",
    "                    if re.match(r'\\*\\*\\*.*?\\*\\*\\*', part):  # Bold Italic\n",
    "                        text = part.strip('*')\n",
    "                        pdf.set_font('Arial', 'BI', 12)\n",
    "                        pdf.write(10, text)\n",
    "                    elif re.match(r'\\*\\*.*?\\*\\*', part):  # Bold\n",
    "                        text = part.strip('*')\n",
    "                        pdf.set_font('Arial', 'B', 12)\n",
    "                        pdf.write(10, text)\n",
    "                    elif re.match(r'\\*.*?\\*', part):  # Italic\n",
    "                        text = part.strip('*')\n",
    "                        pdf.set_font('Arial', 'I', 12)\n",
    "                        pdf.write(10, text)\n",
    "                    elif re.match(r'\\[.*?\\]\\(.*?\\)', part):  # Markdown-style link\n",
    "                        display_text = re.search(r'\\[(.*?)\\]', part).group(1)\n",
    "                        url = re.search(r'\\((.*?)\\)', part).group(1)\n",
    "                        pdf.set_text_color(0, 0, 255)  # Set text color to blue\n",
    "                        pdf.set_font('', 'U')\n",
    "                        pdf.write(10, display_text, url)\n",
    "                        pdf.set_text_color(0, 0, 0)  # Reset text color\n",
    "                        pdf.set_font('Arial', '', 12)\n",
    "                    # elif re.match(r'\\([^ ]+?\\)', part):  # Plain URL\n",
    "                    #     url = part[1:-1]\n",
    "                    #     pdf.set_text_color(0, 0, 255)  # Set text color to blue\n",
    "                    #     pdf.set_font('', 'U')\n",
    "                    #     pdf.write(10, url, url)\n",
    "                    else:\n",
    "                        pdf.write(10, part)\n",
    "                    pdf.set_text_color(0, 0, 0)             # Reset text color\n",
    "                    pdf.set_font('Arial', '', 12)   # Reset font\n",
    "\n",
    "                pdf.ln(10)\n",
    "\n",
    "        pdf.output(filename)\n",
    "        return f\"PDF generated: {filename}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error generating PDF: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Agentic Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a workflow for conducting research on a collectivite, gathering information from various sources (using **Tavily**), and generating a detailed report. It starts by setting up a `ResearchState` data structure, which holds the collectivite's name, keywords, documents retrieved during research, and messages exchanged during the process.\n",
    "\n",
    "The `Citation` and `QuotedAnswer` classes define how citations and answers should be structured when generating the report, ensuring clear and accurate referencing of sources.\n",
    "\n",
    "The `TavilyQuery` and `TavilySearchInput` classes allow for multi-query searches using the **Tavily Search** tool, enabling more precise and efficient information gathering. The `tavily_search` function is an asynchronous tool that performs these searches in parallel, collecting and consolidating the results.\n",
    "\n",
    "The `TavilyExtractInput` class defines the input structure for extracting raw content from URLs using **Tavily Extract**, allowing for more detailed information gathering when needed. This is especially useful in situations where summaries alone are insufficient, and deeper content extraction is required, such as in the case of detailed collectivite research.\n",
    "\n",
    "The workflow is managed by a `StateGraph`, which orchestrates the sequence of operations:\n",
    "\n",
    "- The workflow starts at the `research` node to initiate the research process.\n",
    "- Based on `should_continue`, it either continuing researching with *Tavily Search* or moves to curating the gathered documents using Tavily Extract (`curate` node).\n",
    "- After curating, the workflow continues to `write`, where a detailed report with citations is written.\n",
    "- Finally, `publish` converts the report into a PDF.\n",
    "\n",
    "The workflow involves several key functions:\n",
    "\n",
    "- `tool_node` handles the execution of search tools and stores the results for further processing. It ensures that only new documents are added to the `documents` dictionary, summarizing the findings in a message.\n",
    "  \n",
    "- `research_model` invokes the research model to gather relevant information about the collectivite, based on recent developments in the past week. It uses the `tavily_search` tool to gather documents and provides specific instructions for focusing on the collectivite's keywords. It also determines the next course of action in the workflow. Depending on the information gathered so far, the model decides whether to continue executing the **Tavily Search** tool to gather more data or to proceed to the next step. This decision-making capability is what makes the workflow agentic, allowing it to dynamically adapt to the specific research needs and ensure that the most relevant and comprehensive information is included in the final output.\n",
    "\n",
    "- `should_continue` determines whether to continue using research tools or proceed to curating the gathered information based on the model's decision.\n",
    "\n",
    "- `select_and_process` curates the gathered documents to retain those most relevant to the collectivite. It generates a prompt for a model to filter the documents, excluding those with conflicting information or irrelevant keywords. The relevant documents are stored in `RAG_docs`, and additional raw content is extracted.\n",
    "\n",
    "- `write_report` generates a detailed, in-depth report based on the curated documents (`RAG_docs`). The report is written in Markdown syntax, and includes a citation section with sources formatted as hyperlinks.\n",
    "\n",
    "- `generete_pdf` converts the generated report into a PDF file and saves it to a designated directory for easy sharing and reference.\n",
    "\n",
    "This setup enables an agentic approach to collectivite research, resulting in a well-informed, well-cited report. By utilizing a multi-query strategy, document filtering, and additional extraction from selected documents, the workflow ensures efficient and accurate data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the research state\n",
    "class ResearchState(TypedDict):\n",
    "    collectivite: str\n",
    "    region: str\n",
    "    local_keywords: str\n",
    "    exclude_keywords: str\n",
    "    report: str\n",
    "    # Declare a dictionary where:\n",
    "    # - The outer dictionary has string keys.\n",
    "    # - The inner dictionary can have keys of different types (e.g., str, int).\n",
    "    # - The inner dictionary values can be of different types (e.g., str, float).\n",
    "    documents: Dict[str, Dict[Union[str, int], Union[str, float]]]\n",
    "    RAG_docs: Dict[str, Dict[Union[str, int], Union[str, float]]]\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# Define the structure for the model's response, which includes citations.\n",
    "\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    source_id: str = Field(\n",
    "        ...,\n",
    "        description=\"The URL of a specific source which justifies the answer.\",\n",
    "    )\n",
    "    quote: str = Field(\n",
    "        ...,\n",
    "        description=\"The VERBATIM quote from the specified source that justifies the answer.\",\n",
    "    )\n",
    "\n",
    "    @validator('source_id')\n",
    "    def validate_url(cls, v):\n",
    "        try:\n",
    "            result = urlparse(v)\n",
    "            return v if all([result.scheme, result.netloc]) else None\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "class QuotedAnswer(BaseModel):\n",
    "    \"\"\"Answer the user question based only on the given sources, and cite the sources used.\"\"\"\n",
    "    answer: str = Field(\n",
    "        ...,\n",
    "        description=\"The answer to the user question, which is based only on the given sources. Include any relevant sources in the answer as markdown hyperlinks. For example: 'This is a sample text ([url website](url))'\"\n",
    "    )\n",
    "    citations: List[Citation] = Field(\n",
    "        ...,\n",
    "        description=\"Citations from the given sources that justify the answer.\"\n",
    "    )\n",
    "\n",
    "    @validator('answer')\n",
    "    def validate_answer_has_links(cls, v, values):\n",
    "        if values.get('citations') and not any(f'({c.source_id})' in v for c in values['citations']):\n",
    "            raise ValueError(\"Answer must include markdown links to citations.\")\n",
    "        return v\n",
    "    \n",
    "# Add Tavily's arguments to enhance the web search tool's capabilities\n",
    "class TavilyQuery(BaseModel):\n",
    "    query: str = Field(description=\"web search query\")\n",
    "    topic: str = Field(description=\"Type of search, should be 'general' or 'news'. Choose 'news' ONLY when the collectivite you are searching is publicly traded and is likely to be featured on popular news\")\n",
    "    days: int = Field(description=\"Number of days back to run 'news' search\")\n",
    "    domains: Optional[List[str]] = Field(default=None, description=\"List of domains to include in the research. Useful when trying to gather information from trusted and relevant domains\")\n",
    "\n",
    "    @validator('topic')\n",
    "    def validate_topic(cls, v):\n",
    "        if v not in ['general', 'news']:\n",
    "            raise ValueError(\"Topic must be either 'general' or 'news'.\")\n",
    "        return v\n",
    "\n",
    "    @validator('days')\n",
    "    def validate_days(cls, v):\n",
    "        if v < 0:\n",
    "            raise ValueError(\"Days must be a non-negative integer.\")\n",
    "        return v\n",
    "\n",
    "# Define the args_schema for the tavily_search tool using a multi-query approach, enabling more precise queries for Tavily.\n",
    "class TavilySearchInput(BaseModel):\n",
    "    sub_queries: List[TavilyQuery] = Field(description=\"Set of sub-queries that can be answered in isolation\")\n",
    "\n",
    "    @validator('sub_queries')\n",
    "    def validate_sub_queries(cls, v):\n",
    "        if not v:\n",
    "            raise ValueError(\"At least one sub-query is required.\")\n",
    "        return v\n",
    "\n",
    "class TavilyExtractInput(BaseModel):\n",
    "    urls: List[str] = Field(description=\"List of a single or several URLs for extracting raw content to gather additional information\")\n",
    "\n",
    "    @validator('urls')\n",
    "    def validate_urls(cls, v):\n",
    "        if not v:\n",
    "            raise ValueError(\"At least one URL is required.\")\n",
    "        return v\n",
    "\n",
    "@tool(\"tavily_search\", args_schema=TavilySearchInput, return_direct=True)\n",
    "async def tavily_search(sub_queries: List[TavilyQuery]):\n",
    "    \"\"\"Perform searches for each sub-query using the Tavily search tool concurrently.\"\"\"  \n",
    "    # Define a coroutine function to perform a single search with error handling\n",
    "    async def perform_search(itm):\n",
    "        try:\n",
    "            # Add date to the query as we need the most recent results\n",
    "            query_with_date = f\"{itm.query} {datetime.now().strftime('%m-%Y')}\"\n",
    "            # Attempt to perform the search, hardcoding days to 7 (days will be used only when topic is news)\n",
    "            response = await tavily_client.search(\n",
    "                query=query_with_date,\n",
    "                topic=itm.topic,\n",
    "                days=itm.days,\n",
    "                max_results=4\n",
    "            )\n",
    "            return response['results']\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions, log them, and return an empty list\n",
    "            print(f\"Error occurred during search for query '{itm.query}': {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    # Run all the search tasks in parallel\n",
    "    search_tasks = [perform_search(itm) for itm in sub_queries]\n",
    "    search_responses = await asyncio.gather(*search_tasks)\n",
    "    \n",
    "    # Combine the results from all the responses\n",
    "    search_results = []\n",
    "    for response in search_responses:\n",
    "        search_results.extend(response)\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "# Code for adding Tavily Extract as a tool (found it more useful to use Tavily Extract in a separate node)\n",
    "@tool(\"tavily_extract\", args_schema=TavilyExtractInput, return_direct=True)\n",
    "async def tavily_extract(urls: TavilyExtractInput):\n",
    "    \"\"\"Extract raw content from urls to gather additional information.\"\"\"\n",
    "    try:\n",
    "        response = await tavily_client.extract(urls=urls)\n",
    "        return response['results']\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions, log them, and return an empty list\n",
    "        print(f\"Error occurred during extract: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "\n",
    "tools = [tavily_search]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "tavily_client = AsyncTavilyClient()\n",
    "\n",
    "\n",
    "# Define an async custom research tool node to store Tavily's search results for improved processing and later on filtering\n",
    "async def tool_node(state: ResearchState):\n",
    "    docs = state.get('documents',{})\n",
    "    docs_str = \"\"\n",
    "    msgs = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        new_docs = await tool.ainvoke(tool_call[\"args\"])\n",
    "        for doc in new_docs:\n",
    "            # Make sure that this document was not retrieved before\n",
    "            if not docs or doc['url'] not in docs:\n",
    "                docs[doc['url']] = doc\n",
    "                docs_str += json.dumps(doc)\n",
    "            # For Tavily Extract tool, checking if raw_content was retrieved a document\n",
    "            if doc.get('raw_content', None) and doc['url'] in docs:\n",
    "                docs[doc['url']]['raw_content'] = doc['raw_content'] # add raw content retrieved by extract\n",
    "                docs_str += json.dumps(doc)\n",
    "        msgs.append(ToolMessage(content=f\"Found the following new documents/information: {docs_str}\", tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": msgs, \"documents\": docs}\n",
    "    \n",
    "# Invoke a model with research tools to gather data about the collectivite  \n",
    "def research_model(state: ResearchState):\n",
    "    prompt = f\"\"\"Nous sommes le {datetime.now().strftime('%d/%m/%Y')}.\n",
    "Vous êtes un expert en recherche chargé de rassembler des informations pour un rapport hebdomadaire sur les collectivités locales.\n",
    "Votre objectif actuel est de collecter des documents concernant la collectivité locale suivante : {state['collectivite']} en région {state['region']}.\n",
    "Les mots-clés fournis pour faciliter la recherche sont : {state['local_keywords']}.\n",
    "**Instructions :**\n",
    "- Utilisez l'outil 'tavily_search' pour rechercher des documents pertinents, notamment ceux publiés sur Data.gouv.fr.\n",
    "- Concentrez-vous sur la collecte d’informations récentes concernant le budget principal et les indicateurs d’endettement.\n",
    "- Lorsque vous estimez avoir rassemblé suffisamment d’informations, indiquez \"J'ai rassemblé suffisamment d'informations et je suis prêt(e) à procéder.\"\n",
    "\"\"\"\n",
    "    messages = state['messages'] + [SystemMessage(content=prompt)]\n",
    "    response = llm.bind_tools(tools).invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "    \n",
    "\n",
    "# Define the function that decides whether to continue research using tools or proceed to writing the report\n",
    "def should_continue(state: ResearchState) -> Literal[\"tools\", \"curate\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user with citations)\n",
    "    return \"curate\"\n",
    "\n",
    "async def select_and_process(state: ResearchState):\n",
    "    msg = \"Tri des documents en cours...\\n\"\n",
    "    prompt = f\"\"\"Vous êtes un expert en recherche spécialisé dans l'analyse des collectivités locales.\n",
    "Votre tâche actuelle consiste à examiner une liste de documents et à sélectionner les URLs les plus pertinentes concernant les développements récents de la collectivité suivante : {state['collectivite']}.\n",
    "Votre objectif est de choisir les documents se rapportant à la collectivité correcte et de fournir les informations les plus cohérentes et synchronisées, en vous appuyant sur les mots-clés suivants comme guide : {state['local_keywords']}.\n",
    "\"\"\"\n",
    "     # Optionally include exclusion keywords if provided by the user \n",
    "    if state['exclude_keywords']:\n",
    "        prompt += f\"Si vous trouvez des mots indésirables parmi ces exclusions: {state['exclude_keywords']}, ignorez ces documents.\\n\"\n",
    "    \n",
    "    prompt += f\"\\nDocuments à examiner (extrait de résumé) :\\n{state['documents']}\\n\\n\"\n",
    "\n",
    "    prompt += \"Fournissez une liste des URLs pertinentes dans le format JSON avec la clé 'urls' (par exemple: {\\\"urls\\\": [\\\"url1\\\", \\\"url2\\\"]}).\"\n",
    "\n",
    "    # Use the model to filter documents and obtain relevant URLs structured as TavilyExtractInput\n",
    "    messages = [SystemMessage(content=prompt)]  \n",
    "    relevant_urls = llm.with_structured_output(TavilyExtractInput).invoke(messages)\n",
    "    \n",
    "    # Create a dictionary of relevant documents based on the URLs returned by the model\n",
    "    RAG_docs = {url: state['documents'][url] for url in relevant_urls.urls if url in state['documents']}\n",
    "\n",
    "    try:\n",
    "        # Extract raw content from the selected URLs using the Tavily client\n",
    "        response = await tavily_client.extract(urls=relevant_urls.urls)\n",
    "        \n",
    "        # Save the raw content into the RAG_docs dictionary for each URL\n",
    "        msg += \"Extracted raw content for:\\n\"\n",
    "        for itm in response['results']:\n",
    "            url = itm['url']\n",
    "            msg += f\"{url}\\n\" \n",
    "            raw_content = itm['raw_content']\n",
    "            RAG_docs[url]['raw_content'] = raw_content\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during Tavily Extract request\")\n",
    "        \n",
    "    msg += f\"ֿֿ\\n\\nState of RAG documents that will be used for the report:\\n\\n{RAG_docs}\"\n",
    "        \n",
    "    return {\"messages\": [AIMessage(content=msg)],\"RAG_docs\": RAG_docs}\n",
    "            \n",
    "# Define the function to write the report based on the retrieved documents.\n",
    "def write_report(state: ResearchState):\n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"Nous sommes le {datetime.now().strftime('%d/%m/%Y')}.\n",
    "Vous êtes un expert en analyse financière des collectivités locales.\n",
    "Votre tâche consiste à produire DEUX tableau en markdown qui présente les indicateurs financiers les plus récents pour la collectivité locale : {state['collectivite']}, et de sa metropole. Puis comparer les valeurs de la collectivité (ou metropole) avec la moyenne nationale.\n",
    "Les deux tableaux doivent comporter trois colonnes :\n",
    "- **Vision récapitulative** (le libellé de l'indicateur),\n",
    "- **Client** (la valeur pour la collectivité locale),\n",
    "- **Moyenne nationale** (la valeur de référence nationale).\n",
    "\n",
    "Les lignes du tableau doivent porter sur les indicateurs suivants :\n",
    "1. **Encours total budget principal (en euros et en euros par habitant)**  (exemple : \"110 M€ soit 679 €/hab\")\n",
    "2. **Capacité de désendettement (en années)**  (exemple : \"3,4 ans\")\n",
    "3. **Taux d’endettement (en %)**  (exemple : \"51 %\")\n",
    "4. **Durée apparente de la dette (en années)**  (exemple : \"10,1 ans\")\n",
    "\n",
    "Veuillez extraire les informations les plus récentes disponibles dans les documents suivants et produire le tableau en markdown. Pour chaque indicateur, indiquez la valeur, l’année de référence et fournissez le lien vers la source.\n",
    "Voici les documents sur lesquels vous pouvez vous appuyer :\n",
    "{state['RAG_docs']}\n",
    "\"\"\"\n",
    "    # messages = [state['messages'][-1]] + [SystemMessage(content=prompt)] \n",
    "    # Create a system message with the constructed prompt (no need to include entire chat history)\n",
    "    messages = [SystemMessage(content=prompt)] \n",
    "    response = llm.with_structured_output(QuotedAnswer).invoke(messages)\n",
    "    full_report = response.answer\n",
    "    # Add Citations Section to the report\n",
    "    full_report += \"\\n\\n### Citations\\n\"\n",
    "    for citation in response.citations:\n",
    "        doc = state['RAG_docs'].get(citation.source_id)\n",
    "        full_report += f\"- [{doc.get('title',citation.source_id)}]({citation.source_id}): \\\"{citation.quote}\\\"\\n\"\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [AIMessage(content=f\"Generated Report:\\n{full_report}\")], \"report\": full_report}\n",
    "\n",
    "# def generete_pdf(state: ResearchState):\n",
    "#     directory = \"reports\"\n",
    "#     file_name = f\"{state['collectivite']} Weekly Report {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "#     # Check if the directory exists\n",
    "#     if not os.path.exists(directory):\n",
    "#         # Create the directory\n",
    "#         os.makedirs(directory)\n",
    "#     msg = generate_pdf_from_md(state['report'], filename=f'{directory}/{file_name}.pdf')\n",
    "#     return {\"messages\": [AIMessage(content=msg)]}\n",
    "\n",
    "def generate_markdown(state: ResearchState):\n",
    "    directory = \"reports\"\n",
    "    file_name = f\"{state['collectivite']} Weekly Report {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}.md\"\n",
    "    # Debug: print the report content before writing it\n",
    "    print(\"DEBUG: The report content is:\\n\", state['report'])\n",
    "    # Check if the directory exists; if not, create it.\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    with open(f'{directory}/{file_name}', 'w', encoding='utf-8') as f:\n",
    "        f.write(state['report'])\n",
    "    msg = f\"Markdown generated: {directory}/{file_name}\"\n",
    "    return {\"messages\": [AIMessage(content=msg)]}\n",
    "\n",
    "# Define a graph\n",
    "workflow = StateGraph(ResearchState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"research\", research_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "workflow.add_node(\"curate\", select_and_process)\n",
    "workflow.add_node(\"write\", write_report)\n",
    "workflow.add_node(\"publish\", generate_markdown)\n",
    "# Set the entrypoint as route_query\n",
    "workflow.set_entry_point(\"research\")\n",
    "\n",
    "# Determine which node is called next\n",
    "workflow.add_conditional_edges(\n",
    "    \"research\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# Add a normal edge from `tools` to `research`.\n",
    "# This means that after `tools` is called, `research` node is called next in  order to determine if we should keep  or move to the 'curate' step\n",
    "workflow.add_edge(\"tools\", \"research\")\n",
    "workflow.add_edge(\"curate\",\"write\")\n",
    "workflow.add_edge(\"write\", \"publish\")  # Option in the future, to add another step and filter the documents retrieved using rerhank before writing the report\n",
    "workflow.add_edge(\"publish\", END)  # Option in the future, to add another step and filter the documents retrieved using rerhank before writing the report\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install -U --no-cache-dir  \\\n",
    "#             --config-settings=\"--global-option=build_ext\" \\\n",
    "#             --config-settings=\"--global-option=-I$(brew --prefix graphviz)/include/\" \\\n",
    "#             --config-settings=\"--global-option=-L$(brew --prefix graphviz)/lib/\" \\\n",
    "#             pygraphviz\n",
    "# from IPython.display import Image\n",
    "\n",
    "# Image(app.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectivite = \"Nice\"\n",
    "region = \"\"\n",
    "local_keywords = \"Encours total budget principal Ville, Capacité de désendettement, Taux d’endettement, Durée apparente de la dette\"\n",
    "# (Optional) exclude_keywords: Use this field when you need to differentiate the collectivite from others with the same name in a different industry\n",
    "# or when you want to exclude specific types of documents or information. Leave it as an empty string (\"\") if not needed.\n",
    "exclude_keywords = \"\"\n",
    "# You may uncomment your_additional_guidelines and HumanMessage and update the content with some guidelines of your own\n",
    "# your_additional_guidelines=f\"Note that the {collectivite} is ... / focus on ....\"\n",
    "messages = [\n",
    "    SystemMessage(content=\"Vous êtes un expert en recherche spécialisé dans l'analyse des collectivités locales, pret a commencer la recherche.\")\n",
    "    # ,HumanMessage(content=your_additional_guidelines)\n",
    "]\n",
    "\n",
    "limit = 15\n",
    "count = 0\n",
    "\n",
    "async for s in app.astream({\n",
    "    \"collectivite\": collectivite,\n",
    "    \"region\": region,\n",
    "    \"local_keywords\": local_keywords,\n",
    "    \"exclude_keywords\": exclude_keywords,\n",
    "    \"messages\": messages  # Use the trimmed context here\n",
    "}, stream_mode=\"values\"):\n",
    "    if count >= limit:\n",
    "        break \n",
    "    count += 1\n",
    "    message = s[\"messages\"][-1]\n",
    "    if isinstance(message, tuple):\n",
    "        print(message)\n",
    "    else:\n",
    "        message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "colab": {
   "collapsed_sections": [
    "v1a8tvFOW9hX",
    "XUJuVVjV70y9",
    "2SBektD48VjA"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
